{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar los datos \n",
    "### DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy._core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importamos las librerías necesarias.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error\n",
      "File \u001b[1;32mc:\\Users\\Ikasle\\miniconda3\\envs\\SAA\\Lib\\site-packages\\pandas\\__init__.py:62\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     ArrowDtype,\n\u001b[0;32m     65\u001b[0m     Int8Dtype,\n\u001b[0;32m     66\u001b[0m     Int16Dtype,\n\u001b[0;32m     67\u001b[0m     Int32Dtype,\n\u001b[0;32m     68\u001b[0m     Int64Dtype,\n\u001b[0;32m     69\u001b[0m     UInt8Dtype,\n\u001b[0;32m     70\u001b[0m     UInt16Dtype,\n\u001b[0;32m     71\u001b[0m     UInt32Dtype,\n\u001b[0;32m     72\u001b[0m     UInt64Dtype,\n\u001b[0;32m     73\u001b[0m     Float32Dtype,\n\u001b[0;32m     74\u001b[0m     Float64Dtype,\n\u001b[0;32m     75\u001b[0m     CategoricalDtype,\n\u001b[0;32m     76\u001b[0m     PeriodDtype,\n\u001b[0;32m     77\u001b[0m     IntervalDtype,\n\u001b[0;32m     78\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     79\u001b[0m     StringDtype,\n\u001b[0;32m     80\u001b[0m     BooleanDtype,\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     NA,\n\u001b[0;32m     83\u001b[0m     isna,\n\u001b[0;32m     84\u001b[0m     isnull,\n\u001b[0;32m     85\u001b[0m     notna,\n\u001b[0;32m     86\u001b[0m     notnull,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     Index,\n\u001b[0;32m     89\u001b[0m     CategoricalIndex,\n\u001b[0;32m     90\u001b[0m     RangeIndex,\n\u001b[0;32m     91\u001b[0m     MultiIndex,\n\u001b[0;32m     92\u001b[0m     IntervalIndex,\n\u001b[0;32m     93\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     94\u001b[0m     DatetimeIndex,\n\u001b[0;32m     95\u001b[0m     PeriodIndex,\n\u001b[0;32m     96\u001b[0m     IndexSlice,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     NaT,\n\u001b[0;32m     99\u001b[0m     Period,\n\u001b[0;32m    100\u001b[0m     period_range,\n\u001b[0;32m    101\u001b[0m     Timedelta,\n\u001b[0;32m    102\u001b[0m     timedelta_range,\n\u001b[0;32m    103\u001b[0m     Timestamp,\n\u001b[0;32m    104\u001b[0m     date_range,\n\u001b[0;32m    105\u001b[0m     bdate_range,\n\u001b[0;32m    106\u001b[0m     Interval,\n\u001b[0;32m    107\u001b[0m     interval_range,\n\u001b[0;32m    108\u001b[0m     DateOffset,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     to_numeric,\n\u001b[0;32m    111\u001b[0m     to_datetime,\n\u001b[0;32m    112\u001b[0m     to_timedelta,\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     Flags,\n\u001b[0;32m    115\u001b[0m     Grouper,\n\u001b[0;32m    116\u001b[0m     factorize,\n\u001b[0;32m    117\u001b[0m     unique,\n\u001b[0;32m    118\u001b[0m     value_counts,\n\u001b[0;32m    119\u001b[0m     NamedAgg,\n\u001b[0;32m    120\u001b[0m     array,\n\u001b[0;32m    121\u001b[0m     Categorical,\n\u001b[0;32m    122\u001b[0m     set_eng_float_format,\n\u001b[0;32m    123\u001b[0m     Series,\n\u001b[0;32m    124\u001b[0m     DataFrame,\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\Ikasle\\miniconda3\\envs\\SAA\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Ikasle\\miniconda3\\envs\\SAA\\Lib\\site-packages\\pandas\\_libs\\__init__.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: numpy._core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# Importamos las librerías necesarias.\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargamos los datos.\n",
    "valoraciones = pd.read_json('data/web_reviews.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función .info()\n",
    "## El resultado de `valoraciones.info()` es que contiene un total de **10,261 filas** y **9 columnas**.\n",
    "**Columnas:**\n",
    "- **reviewerID**: ID del revisor, tipo `object` (cadena de texto).\n",
    "- **asin**: ID del producto, tipo `object`.\n",
    "- **reviewerName**: Nombre del revisor, tipo `object`.\n",
    "- **helpful**: Información sobre la utilidad de la reseña (como una lista que muestra votos útiles), tipo `object`.\n",
    "- **reviewText**: Texto de la reseña, tipo `object`.\n",
    "- **overall**: Calificación general del producto (1 a 5), tipo `int64`.\n",
    "- **summary**: Resumen breve de la reseña, tipo `object`.\n",
    "- **unixReviewTime**: Fecha de la reseña en formato de tiempo Unix, tipo `int64`.\n",
    "- **reviewTime**: Fecha de la reseña en formato de texto, tipo `object`.\n",
    "\n",
    "### Información adicional:\n",
    "- **Tipos de datos**: La mayoría de las columnas son de tipo `object` (cadenas de texto), excepto `overall` y `unixReviewTime`, que son enteros (`int64`).\n",
    "- **Uso de memoria**: El DataFrame ocupa aproximadamente **721.6 KB** en memoria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar las 5 primeras filas de la tabla\n",
    "print(valoraciones.head())\n",
    "\n",
    "# Información general de la tabla\n",
    "valoraciones.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .describe\n",
    "El resultado de `valoraciones.describe()` proporciona un resumen estadístico de las columnas **numéricas** en el DataFrame. Ayudan a entender la distribución y la tendencia de las calificaciones junto con los tiempos de revisión:\n",
    "\n",
    "### Interpretación:\n",
    "- **overall**: \n",
    "  - Representa la calificación general del producto.\n",
    "  - La media es de **4.59**, lo cual indica que la mayoría de las valoraciones son altas.\n",
    "  - El valor mínimo es **1** y el máximo es **10**.\n",
    "  \n",
    "- **unixReviewTime**: \n",
    "  - Representa la fecha de la reseña en formato Unix.\n",
    "  - La media es **1.356565e+09**, y el rango va desde **1.204819e+09** hasta **1.405382e+09**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valoraciones.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamos un modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ajustamos la columna 'helpful' para obtener el número de votos útiles\n",
    "valoraciones['helpful_votes'] = valoraciones['helpful'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else 0)\n",
    "\n",
    "# Convertimos todas las columnas de tipo object a valores numéricos\n",
    "for column in valoraciones.select_dtypes(include=['object']).columns:\n",
    "    if column == 'reviewText':\n",
    "        # Nueva característica con la longitud del texto\n",
    "        valoraciones['reviewText_length'] = valoraciones['reviewText'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    elif column == 'summary':\n",
    "        # Nueva característica con la longitud del resumen\n",
    "        valoraciones['summary_length'] = valoraciones['summary'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    else:\n",
    "        # Eliminamos columnas irrelevantes\n",
    "        valoraciones = valoraciones.drop(columns=[column])\n",
    "\n",
    "# Seleccionamos todas las características menos la columna objetivo 'overall'\n",
    "X = valoraciones[['unixReviewTime', 'reviewText_length', 'summary_length']]\n",
    "y = valoraciones['overall']\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Entrenamos un modelo de Random Forest para calcular la importancia de las características\n",
    "modelo_rf = RandomForestClassifier(random_state=1)\n",
    "modelo_rf.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos la importancia de las características\n",
    "importancias = modelo_rf.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Creamos un DataFrame para visualizar la importancia de cada característica\n",
    "importantesDf = pd.DataFrame({'Feature': features, 'Importance': importancias})\n",
    "importantesDf = importantesDf.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Mostrar DataFrame de importancias\n",
    "print(\"\\nImportancia de las características:\")\n",
    "print(importantesDf)\n",
    "\n",
    "# Visualizamos las importancias de características\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importantesDf['Feature'], importantesDf['Importance'], color='skyblue')\n",
    "plt.xlabel('Importancia')\n",
    "plt.title('Importancia de las Características en el Modelo Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Filtramos las características menos relevantes según un umbral de importancia\n",
    "umbral_importancia = 0.01  # Ajusta este valor según los resultados\n",
    "mejores_features = importantesDf[importantesDf['Importance'] > umbral_importancia]['Feature'].tolist()\n",
    "\n",
    "# Creamos el conjunto de datos final solo con las características más relevantes\n",
    "X_train_mejores = X_train[mejores_features]\n",
    "X_test_mejores = X_test[mejores_features]\n",
    "\n",
    "# Entrenamos y evaluamos el modelo con las características seleccionadas\n",
    "modelo_final = RandomForestClassifier(random_state=1)\n",
    "modelo_final.fit(X_train_mejores, y_train)\n",
    "predicciones_final = modelo_final.predict(X_test_mejores)\n",
    "accuracy_final = modelo_final.score(X_test_mejores, y_test)\n",
    "print(f'\\nPrecisión del modelo con características seleccionadas: {accuracy_final:.2f}')\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Definimos los modelos a comparar\n",
    "modelos = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=1),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=1),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=200, random_state=1)\n",
    "}\n",
    "\n",
    "# Evaluamos cada modelo\n",
    "for nombre, modelo in modelos.items():\n",
    "    modelo.fit(X_train, y_train)\n",
    "    predicciones = modelo.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predicciones)\n",
    "    print(f'Precisión de {nombre}: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscar missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino una función que me ayude a comparar las diferentes estrategias.\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    ''' \n",
    "    Función para comparar estrategias y devolver el MAE \n",
    "    '''\n",
    "    model = RandomForestRegressor(n_estimators=73, random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n",
    "\n",
    "# Contamos los valores faltantes en cada columna\n",
    "missing_values = valoraciones.isnull().sum()\n",
    "\n",
    "# Filtramos solo las columnas que tienen valores faltantes\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(\"Valores faltantes por columna:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Identificar columnas con valores faltantes\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "# ************************************************** NO SE PUEDEN ELIMINAR LAS MISSING VALUES PORQUE NO HAY *******************************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de Tratamiento de Valores Faltantes y Preparación de Datos\n",
    "\n",
    "En este documento se detallan las técnicas utilizadas para tratar valores faltantes y preparar los datos para el modelado. Cada técnica tiene sus propias ventajas y limitaciones, y la selección de cada una depende del contexto y las características de los datos.\n",
    "\n",
    "## 1. Imputación Básica con la Media\n",
    "La **imputación básica con la media** consiste en reemplazar los valores faltantes de cada columna por la media de esa columna. Este método es útil cuando los valores están distribuidos simétricamente, ya que la media representa el \"centro\" de la distribución de datos.\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- Sencillo de implementar.\n",
    "- Mantiene el tamaño del conjunto de datos.\n",
    "\n",
    "**Desventajas**:\n",
    "\n",
    "- Puede sesgar los resultados si hay valores extremos.\n",
    "- No conserva la variabilidad en los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación básica con la media\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputación básica (media) como referencia\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_test))\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_test.columns\n",
    "\n",
    "print(\"MAE (imputación con media):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imputación con Extensión Utilizando la Media\n",
    "La **imputación con extensión** consiste en crear indicadores adicionales que registran si un valor era originalmente faltante. Luego, se imputa el valor faltante usando la media, lo que ayuda a conservar la información sobre los datos faltantes y permite al modelo \"saber\" que se ha realizado una imputación.\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- Conserva información sobre la ausencia de datos.\n",
    "- Puede mejorar el rendimiento del modelo si los datos faltantes siguen un patrón.\n",
    "\n",
    "**Desventajas**:\n",
    "\n",
    "- Aumenta el número de características en el conjunto de datos.\n",
    "- Puede agregar complejidad innecesaria si los valores faltantes son aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación con extensión utilizando la media\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_test.copy()\n",
    "\n",
    "# Añadir indicadores de valores faltantes\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputar la media\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_valid_plus))\n",
    "imputed_X_train.columns = X_train_plus.columns\n",
    "imputed_X_valid.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"MAE (imputación con extensión y media):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Imputación con Extensión Utilizando la Mediana\n",
    "La **imputación con extensión utilizando la mediana** es similar al método anterior, pero reemplaza los valores faltantes por la mediana de cada columna en lugar de la media. Es útil cuando los datos tienen una distribución sesgada o contienen valores atípicos, ya que la mediana es menos sensible a los valores extremos.\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- Reduce el sesgo causado por valores atípicos.\n",
    "- Mantiene la robustez en presencia de distribuciones sesgadas.\n",
    "\n",
    "**Desventajas**:\n",
    "\n",
    "- Puede no capturar bien la media si los datos están distribuidos simétricamente.\n",
    "- Al igual que la media, puede reducir la variabilidad original de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación con extensión utilizando la mediana\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_test.copy()\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_valid_plus))\n",
    "imputed_X_train.columns = X_train_plus.columns\n",
    "imputed_X_valid.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"MAE (imputación con extensión y mediana):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Imputación con Extensión Utilizando la Moda\n",
    "La **imputación con extensión utilizando la moda** reemplaza los valores faltantes por el valor más frecuente (o moda) de cada columna. Este método es especialmente útil para datos categóricos, donde la moda representa el valor que ocurre con mayor frecuencia.\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- Muy útil para datos categóricos.\n",
    "- Mantiene la distribución de valores en columnas con poca variabilidad.\n",
    "\n",
    "**Desventajas**:\n",
    "\n",
    "- Puede introducir sesgo si la moda no representa bien los valores faltantes.\n",
    "- Menos adecuado para datos numéricos continuos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación con extensión utilizando la moda\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_test.copy()\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_valid_plus))\n",
    "imputed_X_train.columns = X_train_plus.columns\n",
    "imputed_X_valid.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"MAE (imputación con extensión y moda):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Imputación con Extensión Utilizando un Valor Constante\n",
    "La **imputación con un valor constante** es una técnica donde se reemplazan los valores faltantes con un valor específico (por ejemplo, `0` o `-1`). Este método es útil cuando se desea marcar valores faltantes con un indicador específico o cuando el contexto permite asignar un valor neutro o común.\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- Proporciona flexibilidad para seleccionar el valor de imputación.\n",
    "- Útil para valores categóricos o cuando el valor faltante tiene un significado específico.\n",
    "\n",
    "**Desventajas**:\n",
    "\n",
    "- Puede no ser adecuado si el valor constante no tiene sentido en el contexto de los datos.\n",
    "- Puede afectar negativamente el rendimiento del modelo si el valor elegido introduce ruido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación con extensión utilizando un valor constante\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_test.copy()\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_valid_plus))\n",
    "imputed_X_train.columns = X_train_plus.columns\n",
    "imputed_X_valid.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"MAE (imputación con extensión y valor constante):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Eliminación de Columnas con Valores Faltantes\n",
    "La **eliminación de columnas con valores faltantes** consiste en eliminar cualquier columna que contenga valores faltantes. Es una técnica útil si hay pocas columnas con valores faltantes o si las columnas no son esenciales para el modelo.\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- Sencilla de implementar y asegura que no haya datos faltantes en el conjunto final.\n",
    "- Puede mejorar la eficiencia al reducir el número de características.\n",
    "\n",
    "**Desventajas**:\n",
    "\n",
    "- Puede resultar en la pérdida de información importante si las columnas eliminadas son relevantes.\n",
    "- No es adecuado si muchas columnas contienen valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciono las columnas con valores faltantes en el conjunto de entrenamiento.\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "# Elimino las columnas de los datos de entrenamiento y validación.\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_test.drop(cols_with_missing, axis=1)\n",
    "\n",
    "# Calculo el MAE usando el conjunto de datos con columnas eliminadas.\n",
    "print(\"MAE (eliminar columnas):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen Preproceso de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputacion utilizando la media\n",
    "print(\"MAE (imputación con media):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_test))\n",
    "\n",
    "# Inputacion con extensión utilizando la media\n",
    "print(\"MAE (imputación con extensión y media):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_test))\n",
    "\n",
    "# Inputacion con extensión utilizando la mediana \n",
    "print(\"MAE (imputación con extensión y mediana):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_test))\n",
    "\n",
    "# Imputacion con extension utilizando la moda \n",
    "print(\"MAE (imputación con extensión y moda):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_test))\n",
    "\n",
    "# Calculo el MAE usando un valor constante\n",
    "print(\"MAE (imputación con extensión y valor constante):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_test))\n",
    "\n",
    "# Calculo el MAE usando el conjunto de datos con columnas eliminadas.\n",
    "print(\"MAE (eliminar columnas):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Técnicas de Transformación de Variables Categóricas\n",
    "\n",
    "Al preparar datos para un modelo de machine learning, es importante transformar las variables categóricas en un formato numérico adecuado. A continuación, se describen tres técnicas comunes para trabajar con variables categóricas.\n",
    "\n",
    "## Eliminación de Variables Categóricas\n",
    "\n",
    "La **eliminación de variables categóricas** consiste en descartar aquellas columnas categóricas que no son necesarias o que pueden introducir ruido en el modelo. Esta técnica es útil cuando ciertas variables categóricas no aportan información significativa o cuando el conjunto de datos tiene demasiadas categorías, lo que complica el proceso de codificación.\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- Sencillo de implementar, ya que solo requiere eliminar columnas.\n",
    "- Reduce la dimensionalidad del conjunto de datos, mejorando la eficiencia.\n",
    "\n",
    "**Desventajas**:\n",
    "\n",
    "- Puede perderse información valiosa si las variables eliminadas son relevantes.\n",
    "- No adecuado si se pierden variables importantes para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos solo las columnas numéricas y eliminamos variables categóricas\n",
    "X_numerico = valoraciones.select_dtypes(exclude=['object'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numerico, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Entrenamos un modelo de Random Forest usando solo variables numéricas\n",
    "modelo_rf = RandomForestClassifier(random_state=1)\n",
    "modelo_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluamos el modelo\n",
    "accuracy = modelo_rf.score(X_test, y_test)\n",
    "print(f'Precisión del modelo tras eliminar variables categóricas: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificación Ordinal (Ordinal Encoding)\n",
    "\n",
    "La **codificación ordinal** asigna un valor numérico a cada categoría en una columna, siguiendo un orden. Este método es adecuado para variables categóricas que tienen un orden inherente, como \"Bajo\", \"Medio\" y \"Alto\". Con este enfoque, se asignan valores enteros a cada nivel de la categoría en orden creciente o decreciente.\n",
    "**Ventajas**:\n",
    "\n",
    "- Mantiene el orden natural de las categorías, lo que es importante para variables ordinales.\n",
    "- Sencillo de implementar y no aumenta la dimensionalidad del conjunto de datos.\n",
    "\n",
    "**Desventajas**:\n",
    "\n",
    "- No es adecuado para variables categóricas sin un orden específico, ya que introduce relaciones numéricas que no existen.\n",
    "- Puede causar problemas si el modelo interpreta los valores numéricos como cantidades en lugar de categorías.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Seleccionamos las columnas categóricas\n",
    "categoricas = valoraciones.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# Codificamos las columnas categóricas ordinales\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "categoricas_encoded = pd.DataFrame(ordinal_encoder.fit_transform(categoricas), columns=categoricas.columns)\n",
    "\n",
    "# Combinamos las columnas numéricas y las codificadas ordinalmente\n",
    "X_ordinal = pd.concat([X_numerico.reset_index(drop=True), categoricas_encoded], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "modelo_rf.fit(X_train, y_train)\n",
    "accuracy = modelo_rf.score(X_test, y_test)\n",
    "print(f'Precisión del modelo con codificación ordinal: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación One-Hot (One-Hot Encoding)\n",
    "\n",
    "La **codificación one-hot** convierte cada categoría en una columna binaria, donde se indica la presencia o ausencia de cada categoría con valores de `1` o `0`. Este método es adecuado para variables categóricas sin orden, como \"Rojo\", \"Verde\" y \"Azul\", ya que evita introducir relaciones numéricas entre categorías.\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- Evita introducir relaciones numéricas no deseadas en las variables categóricas sin orden.\n",
    "- Facilita la interpretación del modelo cuando cada categoría tiene su propia columna.\n",
    "\n",
    "**Desventajas**:\n",
    "\n",
    "- Aumenta la dimensionalidad del conjunto de datos, especialmente cuando hay muchas categorías, lo que puede ralentizar el proceso de entrenamiento.\n",
    "- Puede requerir mucho almacenamiento y procesamiento, especialmente para variables con un gran número de categorías.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Inicializamos el codificador One-Hot y aplicamos la codificación\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "categoricas_one_hot = one_hot_encoder.fit_transform(categoricas)\n",
    "\n",
    "# Convertimos el resultado en un DataFrame con nombres de columnas\n",
    "categoricas_one_hot_df = pd.DataFrame(categoricas_one_hot, columns=one_hot_encoder.get_feature_names_out(categoricas.columns))\n",
    "\n",
    "# Combinamos las columnas numéricas y las codificadas con One-Hot\n",
    "X_one_hot = pd.concat([X_numerico.reset_index(drop=True), categoricas_one_hot_df.reset_index(drop=True)], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_one_hot, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "modelo_rf = RandomForestClassifier(random_state=1)\n",
    "modelo_rf.fit(X_train, y_train)\n",
    "accuracy = modelo_rf.score(X_test, y_test)\n",
    "print(f'Precisión del modelo con codificación One-Hot: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "métodos IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
