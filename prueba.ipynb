{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cargar los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Cargamos los datos\n",
    "valoraciones = pd.read_json('data/web_reviews.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del dataset:\n",
      "       reviewerID        asin  \\\n",
      "0  A2IBPI20UZIR0U  1384719342   \n",
      "1  A14VAT5EAX3D9S  1384719342   \n",
      "2  A195EZSQDW3E21  1384719342   \n",
      "3  A2C00NNG1ZQQG2  1384719342   \n",
      "4   A94QU4C90B1AX  1384719342   \n",
      "\n",
      "                                       reviewerName   helpful  \\\n",
      "0  cassandra tu \"Yeah, well, that's just like, u...    [0, 0]   \n",
      "1                                              Jake  [13, 14]   \n",
      "2                      Rick Bennette \"Rick Bennette    [1, 1]   \n",
      "3                          RustyBill \"Sunday Rocker    [0, 0]   \n",
      "4                                     SEAN MASLANKA    [0, 0]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  Not much to write about here, but it does exac...        5   \n",
      "1  The product does exactly as it should and is q...        5   \n",
      "2  The primary job of this device is to block the...        5   \n",
      "3  Nice windscreen protects my MXL mic and preven...        5   \n",
      "4  This pop filter is great. It looks and perform...        5   \n",
      "\n",
      "                                 summary  unixReviewTime   reviewTime  \\\n",
      "0                                   good      1393545600  02 28, 2014   \n",
      "1                                   Jake      1363392000  03 16, 2013   \n",
      "2                   It Does The Job Well      1377648000  08 28, 2013   \n",
      "3          GOOD WINDSCREEN FOR THE MONEY      1392336000  02 14, 2014   \n",
      "4  No more pops when I record my vocals.      1392940800  02 21, 2014   \n",
      "\n",
      "  instrumentType  \n",
      "0         String  \n",
      "1       Keyboard  \n",
      "2     Wind Brass  \n",
      "3                 \n",
      "4       Keyboard  \n",
      "\n",
      "Información del dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10261 entries, 0 to 10260\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   reviewerID      10261 non-null  object\n",
      " 1   asin            10261 non-null  object\n",
      " 2   reviewerName    10261 non-null  object\n",
      " 3   helpful         10261 non-null  object\n",
      " 4   reviewText      10261 non-null  object\n",
      " 5   overall         10261 non-null  int64 \n",
      " 6   summary         10261 non-null  object\n",
      " 7   unixReviewTime  10261 non-null  int64 \n",
      " 8   reviewTime      10261 non-null  object\n",
      " 9   instrumentType  10261 non-null  object\n",
      "dtypes: int64(2), object(8)\n",
      "memory usage: 801.8+ KB\n",
      "None\n",
      "\n",
      "Estadísticas descriptivas de la calificación 'overall':\n",
      "count    10261.000000\n",
      "mean         4.499659\n",
      "std          1.415221\n",
      "min          0.000000\n",
      "25%          4.000000\n",
      "50%          5.000000\n",
      "75%          5.000000\n",
      "max         10.000000\n",
      "Name: overall, dtype: float64\n",
      "\n",
      "Distribución de calificaciones (overall):\n",
      "overall\n",
      "5     6595\n",
      "4     1981\n",
      "3      730\n",
      "0      273\n",
      "2      240\n",
      "10     240\n",
      "1      202\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Longitud promedio de las reseñas:\n",
      "485.9287593801774\n"
     ]
    }
   ],
   "source": [
    "# Visualizamos las primeras filas del dataset\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(valoraciones.head())\n",
    "\n",
    "# Revisamos la estructura de las columnas y el tipo de datos\n",
    "print(\"\\nInformación del dataset:\")\n",
    "print(valoraciones.info())\n",
    "\n",
    "# Calculamos estadísticas descriptivas para la columna 'overall'\n",
    "print(\"\\nEstadísticas descriptivas de la calificación 'overall':\")\n",
    "print(valoraciones['overall'].describe())\n",
    "\n",
    "# Revisión de valores únicos y su distribución en 'overall'\n",
    "print(\"\\nDistribución de calificaciones (overall):\")\n",
    "print(valoraciones['overall'].value_counts())\n",
    "\n",
    "# Análisis de la longitud de los textos en 'reviewText' para entender el contenido\n",
    "valoraciones['review_length'] = valoraciones['reviewText'].apply(len)\n",
    "print(\"\\nLongitud promedio de las reseñas:\")\n",
    "print(valoraciones['review_length'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.8714076960545543\n",
      "\n",
      "Error absoluto medio (MAE) para las categorías: 0.19581100828056502\n"
     ]
    }
   ],
   "source": [
    "# Eliminación de valores nulos en 'overall' y 'reviewText'\n",
    "valoraciones = valoraciones.dropna(subset=['overall', 'reviewText'])\n",
    "\n",
    "# Convertir 'overall' en categorías: 1-2 = negativa, 3 = neutral, 4-5 = positiva\n",
    "def categorize_rating(rating):\n",
    "    if rating <= 2:\n",
    "        return \"negativa\"\n",
    "    elif rating == 3:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"positiva\"\n",
    "\n",
    "valoraciones['sentimiento'] = valoraciones['overall'].apply(categorize_rating)\n",
    "\n",
    "# Definimos el objetivo (target) y la característica (feature)\n",
    "y = valoraciones['sentimiento']\n",
    "\n",
    "# Convertir el texto de 'reviewText' en características numéricas usando TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=100)\n",
    "X_text = tfidf.fit_transform(valoraciones['reviewText']).toarray()\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Crear el modelo de clasificación\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo con accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"\\nAccuracy: {accuracy}\")\n",
    "\n",
    "# También se puede calcular el MAE como referencia\n",
    "mae = mean_absolute_error(y_test.map({'negativa': 0, 'neutral': 1, 'positiva': 2}), \n",
    "                          pd.Series(predictions).map({'negativa': 0, 'neutral': 1, 'positiva': 2}))\n",
    "print(f\"\\nError absoluto medio (MAE) para las categorías: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento de valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valores faltantes en cada característica:\n",
      "reviewerID        0\n",
      "asin              0\n",
      "reviewerName      0\n",
      "helpful           0\n",
      "reviewText        0\n",
      "overall           0\n",
      "summary           0\n",
      "unixReviewTime    0\n",
      "reviewTime        0\n",
      "instrumentType    0\n",
      "review_length     0\n",
      "sentimiento       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verificar valores faltantes en todas las características del DataFrame 'valoraciones'\n",
    "print(\"\\nValores faltantes en cada característica:\")\n",
    "print(valoraciones.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el texto de 'reviewText' en características numéricas usando TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=100)\n",
    "X_text = tfidf.fit_transform(valoraciones['reviewText']).toarray()\n",
    "\n",
    "# Lista para guardar resultados\n",
    "results = []\n",
    "\n",
    "# Función para evaluar el modelo y registrar los resultados\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, technique):\n",
    "    # Crear el modelo de clasificación\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calcular accuracy y MAE\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test.map({'negativa': 0, 'neutral': 1, 'positiva': 2}), \n",
    "                              pd.Series(predictions).map({'negativa': 0, 'neutral': 1, 'positiva': 2}))\n",
    "    \n",
    "    # Guardar los resultados\n",
    "    results.append({\"Technique\": technique, \"Accuracy\": accuracy, \"MAE\": mae})\n",
    "\n",
    "# Separar las características y el target\n",
    "X = X_text\n",
    "y = valoraciones['sentimiento']\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputación con extensión (Media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imputación con la media\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "X_train_mean = imputer_mean.fit_transform(X_train)\n",
    "X_test_mean = imputer_mean.transform(X_test)\n",
    "evaluate_model(X_train_mean, X_test_mean, y_train, y_test, \"Imputación con Media\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputación con extensión (Mediana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Imputación con la mediana\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "X_train_median = imputer_median.fit_transform(X_train)\n",
    "X_test_median = imputer_median.transform(X_test)\n",
    "evaluate_model(X_train_median, X_test_median, y_train, y_test, \"Imputación con Mediana\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputación con extension (Moda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Imputación con la moda\n",
    "imputer_moda = SimpleImputer(strategy='most_frequent')\n",
    "X_train_moda = imputer_moda.fit_transform(X_train)\n",
    "X_test_moda = imputer_moda.transform(X_test)\n",
    "evaluate_model(X_train_moda, X_test_moda, y_train, y_test, \"Imputación con Moda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputacion con extensión (Valor constante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Imputación con un valor constante (ej. 0)\n",
    "imputer_constant = SimpleImputer(strategy='constant', fill_value=0)\n",
    "X_train_constant = imputer_constant.fit_transform(X_train)\n",
    "X_test_constant = imputer_constant.transform(X_test)\n",
    "evaluate_model(X_train_constant, X_test_constant, y_train, y_test, \"Imputación con Valor Constante (0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminación \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Eliminación de valores nulos\n",
    "# Convertimos X_train y X_test a DataFrames para permitir la manipulación de índices\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "# Eliminación de valores nulos y ajuste de índices\n",
    "X_train_dropped = X_train_df.dropna().reset_index(drop=True)\n",
    "y_train_dropped = y_train.iloc[X_train_dropped.index].reset_index(drop=True)\n",
    "X_test_dropped = X_test_df.dropna().reset_index(drop=True)\n",
    "y_test_dropped = y_test.iloc[X_test_dropped.index].reset_index(drop=True)\n",
    "\n",
    "# Evaluar modelo con eliminación de valores nulos\n",
    "evaluate_model(X_train_dropped.values, X_test_dropped.values, y_train_dropped, y_test_dropped, \"Eliminación de Valores Nulos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justificación \n",
    "- Eliminación de Filas: Eliminamos filas con reviewText nulo, ya que sin texto, la reseña no aporta al análisis de sentimientos.\n",
    "- Imputación con Media o Mediana: Útil para características numéricas en las que deseamos preservar el promedio o el valor central del dataset.\n",
    "- Imputación con Moda: Apropiada para características categóricas, ya que reemplaza valores faltantes con el valor más común en la columna.\n",
    "- Imputación con Valor Constante: Indicada para características donde el valor nulo puede ser interpretado como \"ausencia de valor\" (por ejemplo, 0 podría representar la falta de un atributo en algunas características)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Technique  Accuracy       MAE\n",
      "0                Imputación con Media  0.871408  0.195811\n",
      "1              Imputación con Mediana  0.871408  0.195811\n",
      "2                 Imputación con Moda  0.871408  0.195811\n",
      "3  Imputación con Valor Constante (0)  0.871408  0.195811\n",
      "4        Eliminación de Valores Nulos  0.871408  0.195811\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
